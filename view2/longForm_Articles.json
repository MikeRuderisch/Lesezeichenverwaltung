[
    {
        "tag":"DataScience/LLM/2024",
        "category":"Strategy",
        "title": "LLMs and Programming in the first days of 2024",
        "url": "http:\/\/antirez.com\/news\/140",
        "content": "<head><title\/><\/head><body class=\"readabilityBody\" readability=\"298\">\nI'll start by saying that this article is not meant to be a retrospective on LLMs. It's clear that 2023 was a special year for artificial intelligence: to reiterate that seems rather pointless. Instead, this post aims to be a testimony from an individual programmer. Since the advent of ChatGPT, and later by using LLMs that operate locally, I have made extensive use of this new technology. The goal is to accelerate my ability to write code, but that's not the only purpose. There's also the intent to not waste mental energy on aspects of programming that are not worth the effort. Countless hours spent searching for documentation on peculiar, intellectually uninteresting aspects; the efforts to learn an overly complicated API, often without good reason; writing immediately usable programs that I would discard after a few hours. These are all things I do not want to do, especially now, with Google having become a sea of spam in which to hunt for a few useful things.\nMeanwhile, I am certainly not a novice in programming. I am capable of writing code without any aid, and indeed, I do so quite often. Over time, I have increasingly used LLMs to write high-level code, especially in Python, and much less so in C. What strikes me about my personal experience with LLMs is that I have learned precisely when to use them and when their use would only slow me down. I have also learned that LLMs are a bit like Wikipedia and all the video courses scattered on YouTube: they help those with the will, ability, and discipline, but they are of marginal benefit to those who have fallen behind. I fear that at least initially, they will only benefit those who already have an advantage.\nBut let's take it step by step.\n# Omniscient or Parrots?\nOne of the most concerning phenomena of this new wave of novelty and progress in machine learning is the limited ability of AI experts to accept their limited knowledge. Homo sapiens invented neural networks, and then, even more crucially, an algorithm to automatically optimize the parameters of a neural network. Hardware has become capable of training increasingly larger models, and using statistical knowledge about the data to be processed (the priors) and through a lot of trial and error for successive approximations, architectures have been discovered that work better than others. But all in all, neural networks remain quite opaque.\nIn the face of this inability to explain certain emerging capabilities of LLMs, one would have expected more caution from scientists. Instead, many have deeply underestimated LLMs, saying that after all they were nothing more than somewhat advanced Markov chains, capable, at most, of regurgitating extremely limited variations of what they had seen in the training set. Then this notion of the parrot, in the face of evidence, was almost universally retracted.\nAt the same time, much of the enthusiastic masses attributed to LLMs supernatural powers that do not exist in reality. Unfortunately, LLMs can, at most, interpolate in the space represented by the data they have seen during training: and this would already be a lot. In reality, their ability to interpolate is limited (but still astonishing, and also unexpected). Oh, if only the largest LLMs of today could interpolate continuously in the space bounded by all the code they have seen! Even if they would not be able to produce true novelties, they would be able to replace 99% of programmers. The reality is more modest, as it almost always is. An LLM is certainly capable of writing programs that it has not seen in that exact form, showing a certain ability to blend different ideas that appeared in the training set with a certain frequency. It is also clear that this ability has, at the moment, deep limits, and whenever subtle reasoning is required, LLMs fail disastrously. Yet they represent the greatest achievement of AI, from its dawn to today. This seems undeniable.\n# Stupid but All-Knowing\nIt's true: LLMs are capable, at most, of rudimentary reasoning, often inaccurate, many times peppered with hallucinations about non-existent facts. But they have a vast knowledge. In the field of programming, as well as in other fields for which quality data are available, LLMs are like stupid savants who know a lot of things. It would be terrible to do pair programming with such a partner (for me, pair programming is terrible even in the most general terms): they would have nonsensical ideas and we would have to continuously fight to impose our own. But if this erudite fool is at our disposal and answers all the questions asked of them, things change. Current LLMs will not take us beyond the paths of knowledge, but if we want to tackle a topic we do not know well, they can often lift us from our absolute ignorance to the point where we know enough to move forward on our own.\nIn the field of programming, perhaps their ability would have been of very little interest up to twenty or thirty years ago. Back then you had to know a couple of programming languages, the classic algorithms, and those ten fundamental libraries. The rest you had to add yourself, your own intelligence, expertise, design skills. If you had these ingredients you were an expert programmer, able to do more or less everything. Over time, we have witnessed an explosion of frameworks, programming languages, libraries of all kinds. An explosion of complexity often completely unnecessary and unjustified, but the truth is that things are what they are. And in such a context, an idiot who knows everything is a precious ally.\nLet me give you an example: my experiments on machine learning were carried forward for at least a year using Keras. Then for various reasons, I switched to PyTorch. I already knew what an embedding or a residual network was, but I didn't feel like studying PyTorch's documentation step by step (as I had done with Keras, which I learned when ChatGPT did not yet exist). With LLMs, it was very easy to write Python code that used Torch. I just needed to have clear ideas about the model I wanted to put together and ask the right questions.\n# Time for Examples\nI'm not talking about easy things like: \"Hey, what's the method of class X to do Y\"? If it were just for that, one might be tempted to agree with those who are skeptical about LLMs. What the more complex models are capable of is much more elaborate. Until a few years ago, it would have been pure magic. I can tell GPT4: look, this is the neural network model I have implemented in PyTorch. These are my batches. I would like to resize the tensors so that the function that emits the batches is compatible with the input of the neural network, and I would like to represent things in this particular way. Can you show me the code needed to do the reshaping? GPT4 writes the code, and all I had to do was test in the Python CLI if the tensors really have the dimensions that are useful to me and if the data layout is correct.\nHere's another example. Some time ago I had to implement a BLE client for certain ESP32-based devices. After some research, I realized that multi-platform Bluetooth programming bindings are more or less all unusable. The solution was simple, write the code in Objective C using macOS's native API. So, I found myself having to deal with two problems at the same time: learning the cumbersome BLE API of Objective C, full of patterns that I consider nonsensical (I'm a minimalist, that kind of API is at the opposite end of the spectrum of what I consider \"good design\") and at the same time remembering how to program in Objective C. The last time I had written a program in Objective C was ten years ago: I didn't remember the details of the event loop, memory management, and much more.\nThe final result is this code here, not exactly beautiful, but it does what it has to do. I wrote it in an extremely short time. It would have been impossible otherwise.\n<a rel=\"nofollow\" href=\"https:\/\/github.com\/antirez\/freakwan\/blob\/main\/osx-bte-cli\/SerialBTE.m\">https:\/\/github.com\/antirez\/freakwan\/blob\/main\/osx-bte-cli\/SerialBTE.m<\/a>\nThe code was written mostly by doing cut &amp; paste on ChatGPT of the things I wanted to do and didn't quite know how to do, so they didn't work properly. Having the LLM explain to me what the problem was and how to solve it. It's true that the LLM didn't write much of that code, but it's also true that it significantly accelerated the writing. Would I have been able to do it without ChatGPT? Certainly yes, but the most interesting thing is not the fact that it would have taken me longer: the truth is that I wouldn't even have tried, because it wouldn't have been worth it. This fact is crucial. The ratio between the effort and the benefit of writing such a program, secondary to my project, would have been inconvenient. Moreover, this had a much more useful secondary collateral effect than the program itself: for that project I modified linenoise (one of my libraries for line editing) so that it works in multiplexing.\nAnother example, this time less about code writing and more about data interpretation. I wanted to set up a Python script using a convolutional neural network I found online, but it was quite lacking in documentation. The network had the advantage of being in ONNX format, so I could easily extract a list of inputs and outputs, and their assigned names. I only knew one thing about this convnet: it detected certain features within an image. I didn't know the input image format and size, and especially, the network's output was far more complicated than I imagined (I thought it was a binary classifier: is the observed image okay or does it have problems? Two outputs, but there were hundreds). I began by copy-pasting the ONNX network metadata output into ChatGPT. I explain to the assistant what little I know about the network. ChatGPT hypothesizes how the inputs are organized, and that the outputs are probably normalized boxes indicating parts of the images corresponding to potential defects, and other outputs indicating the likelihood of these defects. After a few minutes of back-and-forth, I had a Python script capable of network inference, plus the necessary code to transform the starting image into the tensor suitable for input, and so on. What struck me about that session was ChatGPT finally \u201cunderstood\u201d how the network functioned once it observed the raw output values (the logits, basically) on a test image: a series of floating-point numbers provided the context to identify the exact output details, the normalization, if the boxes where centred or if the left-top corner was specified, and so forth.\n# Disposable Programs\nI could document dozens of such cases I've narrated above. It would be pointless, as it's the same story repeating itself in more or less the same way. I have a problem, I need to quickly know something that *I can verify* if the LLM is feeding me nonsense. Well, in such cases, I use the LLM to speed up my need for knowledge.\nHowever, there are different cases where I let the LLM write all the code. For example, whenever I need to write a more or less disposable program. Like this one:\n<a rel=\"nofollow\" href=\"https:\/\/github.com\/antirez\/simple-language-model\/blob\/main\/plot.py\">https:\/\/github.com\/antirez\/simple-language-model\/blob\/main\/plot.py<\/a>\nI needed to visualize the loss curve during the learning of a small neural network. I showed GPT4 the format of the CSV file produced by the PyTorch program during learning, and then I requested that if I specified multiple CSV files on the command line, I didn\u2019t want the training and validation loss curves of the same experiment anymore, but a comparison of the validation loss curves of different experiments. The above is the result, as generated by GPT4. Thirty seconds in total.\nSimilarly, I needed a program that read the AirBnB CSV report and grouped my apartments by month and year. Then, considering the cleaning costs, and the number of nights per booking, it would do statistics on the average rental price for different months of the year. This program is extremely useful for me. At the same time, writing it is deadly boring: there's nothing interesting. So I took a nice piece of the CSV file and did copy-paste on GPT4. I wrote to the LLM what the problem was to be solved: the program worked on the first try. I show it to you in full below.<pre>\n```python\nimport pandas as pd\npd.set_option('display.max_rows', None)\ndf = pd.read_csv('listings.csv')\nreservations = df[df['Type'] == 'Reservation']\nreservations['Start Date'] = pd.to_datetime(reservations['Start Date'])\nreservations['Year'] = reservations['Start Date'].dt.year\nreservations['Month'] = reservations['Start Date'].dt.month\nreservations['Nightly Rate'] = (reservations['Amount'] - reservations['Cleaning Fee']) \/ reservations['Nights']\nall_listings = reservations['Listing'].unique()\nall_years = reservations['Year'].unique()\nall_months = range(1, 13)\nindex = pd.MultiIndex.from_product([all_listings, all_years, all_months], names=['Listing', 'Year', 'Month'])\nall_data = pd.DataFrame(index=index).reset_index()\nmerged_data = pd.merge(all_data, reservations, on=['Listing', 'Year', 'Month'], how='left')\naverage_nightly_rates = merged_data.groupby(['Listing', 'Year', 'Month'])['Nightly Rate'].mean().fillna(0)\nprint(average_nightly_rates)\n```\n</pre>To understand exactly how to group the data, which were otherwise quite scattered and disordered, requires some simple reasoning: it\u2019s not a problem that an LLM could solve simply by regurgitating something that approximates the language it has seen. GPT4 has certainly observed similar programs during its training, but almost certainly has not seen a program that had to group things in the exact way I asked for and in a CSV file with that specific format. This shows (in my opinion) that LLMs can interpolate to some extent within the space outlined by the different programs in the training set.\nWriting this type of program would be, in my opinion, an imprudent use of my time. The fact that an LLM can do it for me certainly speeds up my ability to produce code in a rather direct way: I can focus on the things that matter.\n# When things don't go as they should: System Programming\nDespite my success with LLMs and programming, when I write a program in C, I almost always end up using LLMs only as a more convenient form of documentation. I am an expert in system programming, and in that context, LLMs would be useful to me only if they were capable of complex reasoning: in this scenario, they almost always fail.\nLet's start with the prompt of this experiment:\nOutline the requirements of a nice, small but working C implementation of a bloom filter. Focus on the important aspects of the hashing function. Then write it, in good quality C.\nAlso consider that your example implementation should be sized to store 100,000 elements, with a false positive probability of at most 5%. The elements added are null-terminated strings.\nHere\u2019s the output:<pre>\n```c\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;math.h&gt;\n#define N 100000  \/\/ Number of elements\n#define P 0.05    \/\/ False positive probability\n#define M (int)(-(N * log(P)) \/ (log(2) * log(2)))  \/\/ Size of bit array\n#define K (int)(M \/ N * log(2))  \/\/ Number of hash functions\nunsigned char bitArray[M \/ 8 + 1];\nvoid setBit(int n) {\n    bitArray[n \/ 8] |= 1 &lt;&lt; (n % 8);\n}\nint getBit(int n) {\n    return bitArray[n \/ 8] &amp; (1 &lt;&lt; (n % 8));\n}\n\/\/ Basic hash functions (for demonstration purposes)\nint hash1(char *str) {\n    int hash = 5381;\n    int c;\n    while ((c = *str++))\n        hash = ((hash &lt;&lt; 5) + hash) + c;\n    return abs(hash % M);\n}\nint hash2(char *str) {\n    unsigned long hash = 0;\n    int c;\n    while ((c = *str++))\n        hash = c + (hash &lt;&lt; 6) + (hash &lt;&lt; 16) - hash;\n    return abs(hash % M);\n}\nvoid add(char *str) {\n    setBit(hash1(str));\n    setBit(hash2(str));\n    \/\/ Add more hash functions if needed\n}\nint is_in(char *str) {\n    return getBit(hash1(str)) &amp;&amp; getBit(hash2(str));\n    \/\/ Check more hash functions if used\n}\nint main() {\n    char *testStr = \"testString\";\n    add(testStr);\n    if (is_in(testStr)) {\n        printf(\"%s is probably in the set.\\n\", testStr);\n    } else {\n        printf(\"%s is definitely not in the set.\\n\", testStr);\n    }\n    return 0;\n}\n```\n</pre>The implementation produced by GPT4 is not exactly brilliant. Bloom filters are quite widespread. It\u2019s not such an exotic data structure. However, it\u2019s clear that writing a decent bloom filter requires a greater capacity for abstraction: for example, there\u2019s the problem of finding an effective way to hash the same string N times, and being certain that the hashes are sufficiently decorrelated.\nYet if you explicitly ask GPT4 to modify the hashing function so that it produces N decorrelated outputs, its proposed solution is significantly more acceptable:<pre>\n```c\nunsigned int hash(const char* str, unsigned int hash_id) {\n    unsigned int hash = 5381;\n    int c;\n    while ((c = *str++)) {\n        hash = ((hash &lt;&lt; 5) + hash) + c + hash_id;\n    }\n    return hash % BF_SIZE;\n}\n```\n</pre>If it had come up with this idea on its own, it would have written the bloom filter differently, setting K bits at a time using a single hashing function.\nGPT4 was able to perform, in isolation, the task of writing an appropriate and more generic hashing function. But in the larger project of writing a bloom filter, it failed to show sufficient reasoning ability, and instead spat out two different but similar hashing functions: this is what one would expect from a poorly written article example on bloom filters.\nThe current weak reasoning abilities of Large Language Models, combined with a possible scarcity of sources on the subject, and even worse, potentially many low-quality sources, collectively result in this meager outcome. Yet, this is not an isolated case. The few times I attempt to use LLMs in the realm of algorithmic or system programming, the results are extremely poor, even when the expected reasoning abilities are not greater than what LLMs seem to possess in contexts like Python programming.\nAt the same time, GPT-4 is capable of decompiling the function it produced (in a separate session), and also precisely understand its purpose:\n<a rel=\"nofollow\" href=\"https:\/\/chat.openai.com\/share\/f54b64d7-d19a-4be4-8f10-08553aa812f1\">https:\/\/chat.openai.com\/share\/f54b64d7-d19a-4be4-8f10-08553aa812f1<\/a>\nTherefore, stating that there are no uses in system programming contexts is certainly inaccurate.\nOne interesting and pretty much expected thing is that, in these cases, the difference between smaller and larger models is indeed significant. Let\u2019s see how two local models fare.\nFor instance, Mixtral, the MoE of Mistral, responded to the same question as follows:<pre>\n```c\nunsigned int hash(const char* str, unsigned int hash_id) {\n    unsigned int hash = 5381;\n    int c;\n    while ((c = *str++)) {\n        hash = ((hash &lt;&lt; 5) + hash) + c;\n    }\n    return hash + hash_id;\n}\n```\n</pre>Clearly, adding `hash_id` only at the end is an extremely poor solution. Mixtral is an excellent model for many uses, but as for the already weak reasoning capabilities of LLMs, the rule so far is certainly: larger is better.\nThis is a chat with the local LLM deepseek-coder, quantized to 4 bits as I don\u2019t have enough RAM to use the model at higher precision. Yet, being 34 billion parameters large, its reasoning capabilities on this matter seem quite advanced. Here\u2019s the log of the chat (conducted with llama.cpp):\n&gt; I have a hash function that should output N distinct hashes for the same data. However, I'm seeing a very poor distribution of the produced hashes as I change the value of hash_id:<pre>\nunsigned int hash(const char* str, unsigned int hash_id) {\n    unsigned int hash = 5381;\n    int c;\n    while ((c = *str++)) {\n        hash = ((hash &lt;&lt; 5) + hash) + c;\n    }\n    return hash + hash_id;\n}</pre>\nMaybe the salting strategy I'm using is poor? How could I change it?\n[Deep-seek coder reply]\nA possible issue with your implementation is that you're simply adding `hash_id` to the end of the hash. This can potentially lead to a poor distribution, especially if `hash_id` is too large or is not evenly distributed.\nConsider using bitwise operations for mixing the `hash_id` in. Here is an example with XOR:<pre>\n```c\nunsigned int hash(const char* str, unsigned int hash_id) {\n    unsigned int hash = 5381;\n    int c;\n    while ((c = *str++)) {\n        hash = ((hash &lt;&lt; 5) + hash) ^ c + hash_id; \/\/ Using XOR to mix in the hash_id\n     }\n    return (hash ^ hash_id); \/\/ Mixing the hash_id one more time at the end\n}\n```\n</pre>This will spread `hash_id` throughout your hashes better than simply adding it, which can lead to less uniform distribution. [Snipped\u2026 the actual reply contained more informative stuff]\nNot bad for a model running on my MacBook M1 Max. It also mixed the sums with XORs. In this case, the model was certainly helped by the fact that I provided clues about the problem to solve, but it was the model that identified what the real source of the trouble was, and the proposed alternative is valid. Something like the above is not within the reach of any book, documentation, or Google search. And as primitive, a result of interpolation, or however you want to perceive it, the model has performed some form of reasoning, if by reasoning, in this specific case, we accept the identification of the origin of a problem and its potential solution. However one wants to think about LLMs, stating that they are not helpful for programmers is extremely rash.\nAt the same time, however, my experience over the past few months suggests that for system programming, LLMs almost never provide acceptable solutions if you are already an experienced programmer. Let me show you another real world example. My current project, ggufflib, involves writing a library that reads and writes GGUF format files, which is the format in which llama.cpp loads quantized models. Initially, to understand how the quantization encodings worked (for speed reasons the bits of each quant are stored in fancy ways), I tried using ChatGPT, but then I resolved to reverse engineer llama.cpp's code: it was much faster. An LLM that can decently assist a system programmer, if it sees the data encoding \u201cstruct\u201d declaration and the decoding function, should be able to reconstruct the data format documentation. The functions of llama.cpp were small enough to fit entirely in the context of GPT4, yet the output was completely useless. In these cases, things are done as in the past: paper and pen, reading the code, and seeing where the bits that the decoder extracts are registered.\nLet me explain better the above use case so that you can try it yourself, if you wish. We have this structure from llama.cpp implementation.<pre>\n\/\/ 6-bit quantization\n\/\/ weight is represented as x = a * q\n\/\/ 16 blocks of 16 elements each\n\/\/ Effectively 6.5625 bits per weight\ntypedef struct {\n    uint8_t ql[QK_K\/2];      \/\/ quants, lower 4 bits\n    uint8_t qh[QK_K\/4];      \/\/ quants, upper 2 bits\n    int8_t  scales[QK_K\/16]; \/\/ scales, quantized with 8 bits\n    ggml_fp16_t d;           \/\/ super-block scale\n} block_q6_K;\nThen there is this function that is used to perform the dequantization:\nvoid dequantize_row_q6_K(const block_q6_K * restrict x, float * restrict y, int k) {\n    assert(k % QK_K == 0);\n    const int nb = k \/ QK_K;\n    for (int i = 0; i &lt; nb; i++) {\n        const float d = GGML_FP16_TO_FP32(x[i].d);\n        const uint8_t * restrict ql = x[i].ql;\n        const uint8_t * restrict qh = x[i].qh;\n        const int8_t  * restrict sc = x[i].scales;\n        for (int n = 0; n &lt; QK_K; n += 128) {\n            for (int l = 0; l &lt; 32; ++l) {\n                int is = l\/16;\n                const int8_t q1 = (int8_t)((ql[l +  0] &amp; 0xF) | (((qh[l] &gt;&gt; 0) &amp; 3) &lt;&lt; 4)) - 32;\n                const int8_t q2 = (int8_t)((ql[l + 32] &amp; 0xF) | (((qh[l] &gt;&gt; 2) &amp; 3) &lt;&lt; 4)) - 32;\n                const int8_t q3 = (int8_t)((ql[l +  0]  &gt;&gt; 4) | (((qh[l] &gt;&gt; 4) &amp; 3) &lt;&lt; 4)) - 32;\n                const int8_t q4 = (int8_t)((ql[l + 32]  &gt;&gt; 4) | (((qh[l] &gt;&gt; 6) &amp; 3) &lt;&lt; 4)) - 32;\n                y[l +  0] = d * sc[is + 0] * q1;\n                y[l + 32] = d * sc[is + 2] * q2;\n                y[l + 64] = d * sc[is + 4] * q3;\n                y[l + 96] = d * sc[is + 6] * q4;\n            }\n            y  += 128;\n            ql += 64;\n            qh += 32;\n            sc += 8;\n        }\n    }\n}\n</pre>If I ask GPT4 to write an outline of the format used, it struggles to provide a clear explanation of how the blocks are stored on the lower \/ upper 4 bits of \u201cql\u201d depending on the weight position. For this blog post, I also tried asking it to write a simpler function that shows how data is stored (maybe it can\u2019t explain it with words, but can with code). The produced function is broken in many ways, the indexes are wrong, the 6-bit -&gt; 8-bit sign extension is wrong (it just casts to uint8_t), and so forth.\nBtw, this is the code that I ended writing myself:<pre>\n    } else if (tensor-&gt;type == GGUF_TYPE_Q6_K) {\n        uint8_t *block = (uint8_t*)tensor-&gt;weights_data;\n        uint64_t i = 0; \/\/ i-th weight to dequantize.\n        while(i &lt; tensor-&gt;num_weights) {\n            float super_scale = from_half(*((uint16_t*)(block+128+64+16)));\n            uint8_t *L = block;\n            uint8_t *H = block+128;\n            int8_t *scales = (int8_t*)block+128+64;\n            for (int cluster = 0; cluster &lt; 2; cluster++) {\n                for (uint64_t j = 0; j &lt; 128; j++) {\n                    f[i] = (super_scale * scales[j\/16]) *\n                           ((int8_t)\n                            ((((L[j%64] &gt;&gt; (j\/64*4)) &amp; 0xF) |\n                             (((H[j%32] &gt;&gt; (j\/32*2)) &amp; 3) &lt;&lt; 4)))-32);\n                    i++;\n                    if (i == tensor-&gt;num_weights) return f;\n                }\n                L += 64;\n                H += 32;\n                scales += 8;\n            }\n            block += 128+64+16+2; \/\/ Go to the next block.\n        }\n   }\n</pre>From the function above, I removed what was the actual contribution of this code: the long comments documenting the exact format used by llama.cpp Q6_K encoding. Now, it would be immensely useful if GPT could do this for me, and I bet it\u2019s just a matter of months, because these kind of tasks are within what can be reached without any breakthrough, just with a bit of scaling.\n# Putting Things in Perspective\nI regret to say it, but it's true: most of today's programming consists of regurgitating the same things in slightly different forms. High levels of reasoning are not required. LLMs are quite good at doing this, although they remain strongly limited by the maximum size of their context. This should really make programmers think. Is it worth writing programs of this kind? Sure, you get paid, and quite handsomely, but if an LLM can do part of it, maybe it's not the best place to be in five or ten years.\nAnd then, do LLMs have some reasoning abilities, or is it all a bluff? Perhaps at times, they seem to reason only because, as semioticians would say, the \"signifier\" gives the impression of a meaning that actually does not exist. Those who have worked enough with LLMs, while accepting their limits, know for sure that it cannot be so: their ability to blend what they have seen before goes well beyond randomly regurgitating words. As much as their training was mostly carried out during pre-training, in predicting the next token, this goal forces the model to create some form of abstract model. This model is weak, patchy, and imperfect, but it must exist if we observe what we observe. If our mathematical certainties are doubtful and the greatest experts are often on opposing positions, believing what one sees with their own eyes seems a wise approach.\nFinally, what sense does it make today not to use LLMs for programming? Asking LLMs the right questions is a fundamental skill. The less it is practiced, the less one will be able to improve their work thanks to AI. And then, developing a descriptive ability of problems is also useful when talking to other human beings. LLMs are not the only ones who sometimes don't understand what we want to say. Communicating poorly is a great limitation, and many programmers communicate very poorly despite being very capable in their specific field. And now Google is unusable: using LLMs even just as a compressed form of documentation is a good idea. For my part, I will continue to make extensive use of them. I have never loved learning the details of an obscure communication protocol or the convoluted methods of a library written by someone who wants to show how good they are. It seems like \"junk knowledge\" to me. LLMs save me from all this more and more every day.<\/pre>\n<\/body>",
        "created_at": "2024-01-04T08:23:35+01:00"
    },
    {
        "title": "Amazon's Silent Sacking",
        "category":"Strategy",
        "tag":"Management/Amazon",
        "url": "https:\/\/justingarrison.com\/blog\/2023-12-30-amazons-silent-sacking\/",
        "content": "<div alt=\"article\"><h5 class=\"flex items-center text-sm\"><svg xmlns=\"http:\/\/www.w3.org\/2000\/svg\" class=\"mr-1\" width=\"16\" height=\"16\" viewbox=\"0 0 24 24\" stroke-width=\"1.5\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><rect x=\"4\" y=\"5\" width=\"16\" height=\"16\" rx=\"2\"\/><line x1=\"16\" y1=\"3\" x2=\"16\" y2=\"7\"\/><line x1=\"8\" y1=\"3\" x2=\"8\" y2=\"7\"\/><line x1=\"4\" y1=\"11\" x2=\"20\" y2=\"11\"\/><rect x=\"8\" y=\"15\" width=\"2\" height=\"2\"\/><\/svg> Posted on December 30, 2023 \u00a0\u2022\u00a0 <svg xmlns=\"http:\/\/www.w3.org\/2000\/svg\" class=\"mr-1\" width=\"16\" height=\"16\" viewbox=\"0 0 24 24\" stroke-width=\"1.5\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><circle cx=\"12\" cy=\"12\" r=\"9\"\/><\/svg> 6\u00a0minutes \u00a0\u2022 <svg xmlns=\"http:\/\/www.w3.org\/2000\/svg\" class=\"mx-1\" width=\"16\" height=\"16\" viewbox=\"0 0 24 24\" stroke-width=\"1.5\" stroke=\"currentColor\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><line x1=\"3\" y1=\"6\" x2=\"3\" y2=\"19\"\/><line x1=\"12\" y1=\"6\" x2=\"12\" y2=\"19\"\/><line x1=\"21\" y1=\"6\" x2=\"21\" y2=\"19\"\/><\/svg> 1187\u00a0words<\/h5><p><em>This is not financial advice and I am not an expert.<\/em><\/p><p>Amazon has a stock problem. It\u2019s not only Amazon, but they have more to lose. If you\u2019re a customer, that\u2019s going to be bad for your business too.<\/p><p>In 2023 Amazon laid off more than 27,000 people. While that\u2019s a big number it\u2019s a deceptively small percentage for a company with more than 1.6 million employees (1.7%).<\/p><p>The vast majority of those layoffs happened in retail where a majority of Amazon\u2019s employees work. When layoffs <a href=\"https:\/\/www.businessinsider.com\/amazon-layoffs\" target=\"_blank\" rel=\"noopener\">hit AWS<\/a> it was mostly areas that were not revenue generating or had lower margins.<\/p><p>But publicly laying off 27,000 isn\u2019t good for business\u2013at least not immediately.<\/p><p><img src=\"https:\/\/justingarrison.com\/img\/amazon-stock-graph-2023.jpg\" height=\"100%\" width=\"100%\" alt=\"A graph of Amazon&amp;rsquo;s stock price\" referrerpolicy=\"no-referrer\"\/><\/p><p>The low stock at the beginning of the year is before the mass layoffs when operating expenses were high. Then Andy <a href=\"https:\/\/www.aboutamazon.com\/news\/company-news\/andy-jassy-update-on-amazon-return-to-office\" target=\"_blank\" rel=\"noopener\">announced return to office (RTO) initiatives<\/a> , but nothing changed.<\/p><p>I was told repeatedly it wouldn\u2019t affect me or the teams I worked with. Then in the summer that changed.<\/p><p>The negative press associated with layoffs wasn\u2019t good. But the most effective way to reduce operational expenses was to get rid of all the expensive people. How could they force people to leave without severance packages or en masse? Making them miserable and silently sacking them.<\/p><p>So RTO was enforced. And people started leaving in droves. If they weren\u2019t leaving they were looking, or at minimum waiting for their next RSU payout.<\/p><p>In my small sphere of people there wasn\u2019t a single person under an L7 that didn\u2019t want out. From what I could gather this mostly came down to compensation.<\/p><p>Independent contributors (IC) and managers that are L7 or above generally make $400k-$800k and for that much money they\u2019re willing to put up with some inconveniences. Since Amazon\u2019s pay is roughly 40% stock, they only make that much money so long as the stock stays up.<\/p><p>If Amazon keeps lowering their operating costs their earnings go up and stock rises. At the cost of burning out everyone doing the work.<\/p><h2 id=\"welcome-to-day-2\">Welcome to Day 2<\/h2><p>As customers are cutting their own costs, $1 spent on AWS is worth less than it was last year. Every trend line still goes up and to the right, but growth is slowing.<\/p><p>Customers aren\u2019t coming to the cloud for VMs and ludicrously expensive network anymore. They want higher levels of abstraction that AWS has historically been terrible at delivering.<\/p><p>Couple that with <a href=\"https:\/\/www.lastweekinaws.com\/blog\/aws-degenerative-ai-blunder\/\" target=\"_blank\" rel=\"noopener\">Amazon trailing in AI<\/a> and the most effective way Amazon can grow is by reducing costs. The biggest cost is people.<\/p><p>Many teams at Amazon have been in a hiring freeze for over a year. And now they\u2019re chasing away the people they do have.<\/p><p>Amazon has shifted from a leader to a follower. From my perspective it\u2019s not going well.<\/p><p>Amazon hasn\u2019t put in the decade of AI research Google has. It doesn\u2019t partner with external companies as good as Microsoft. The high margin services AWS was built on (e.g. network egress) is being <a href=\"https:\/\/www.cloudflare.com\/developer-platform\/r2\/\" target=\"_blank\" rel=\"noopener\">given away for free<\/a> by competitors.<\/p><p>Amazon is good at identifying real world problems they\u2019ve faced from running an extremely large online store and logistics. Generative AI hasn\u2019t been a problem Amazon has identified needed to be fixed (or even deeply worked on) until it was costing them business deals.<\/p><h2 id=\"no-more-pizza-teams\">No more pizza teams<\/h2><p>When I started at Amazon I was impressed that service teams were independent. It was the <a href=\"https:\/\/martinfowler.com\/bliki\/TwoPizzaTeam.html\" target=\"_blank\" rel=\"noopener\">purest implementation of devops<\/a> I had never seen before. Especially at Amazon\u2019s scale.<\/p><p>Then I realized how expensive it is. There are only a handful of centralized teams at Amazon. Those are almost all tools and compliance teams.<\/p><p>Pipelines, SDKs, and security are centralized. All components of a service team are self-contained as part of that team. It turns out that devops is a very expensive org chart.<\/p><p>Many of the service teams have lost a lot of institutional knowledge as part of RTO. Teams were lean before 2023, now they\u2019re emaciated.<\/p><p>Teams can\u2019t keep innovating when they\u2019re just trying to keep the lights on. They can\u2019t maintain on-call schedules without the ability to take vacation or sick days.<\/p><p>The next logical step to reduce costs is to centralize expertise. It\u2019s the reason many large companies have database administration, network engineering, or platform teams.<\/p><p>They\u2019ll have to give up on autonomy to reduce duplication. Amazon has never had a platform engineering team or site reliability engineers (SRE). I suspect in 2024 they\u2019ll start to reorg into a more centralized friendly org chart.<\/p><p>They won\u2019t call the teams that because those titles come from Google, but they\u2019ll effectively be the same. They\u2019ll centralize to create \u201cbetter collaboration\u201d but in reality it\u2019ll be because they can\u2019t lower margins enough to keep their earnings calls positive.<\/p><p>I suspect there\u2019ll be a major AWS outage in 2024. No amount of multi-region redundancy will protect you.<\/p><p>There has already been <a href=\"https:\/\/www.linkedin.com\/posts\/brucebawest_the-new-frequency-of-large-scale-events-at-activity-7109656617100484608-uGmJ\" target=\"_blank\" rel=\"noopener\">an increase in large scale events (LSE) throughout Amazon<\/a> , but AWS is so big most customers don\u2019t notice. This is a direct result of RTO and Amazon\u2019s silent sacking of thousands of people.<\/p><p>Amazon isn\u2019t incentivized to publicly share LSEs. Only outages customers notice are worthy of a dashboard update, but even those are quickly swept under the \u201call greens\u201d dashboard.<\/p><p>Amazon is an operationally strong company. Much stronger than any company I\u2019ve worked for before.<\/p><p>But those operational practices depend on people. When people are eliminated to raise the stock price bar, operational practices suffer.<\/p><h2 id=\"amazon-wont-fire-me\">Amazon won\u2019t fire me<\/h2><p>On September 1st, 2023 I was told by my skip level manager and VP that my team and an adjacent team were being eliminated. They claimed we all did such good work that they wanted us to remain at Amazon. \u201cWe still have a job, just not a role.\u201d<\/p><p>I was skeptical of how it was communicated\u2013or rather not communicated\u2013by management and I asked if severance was an option. I was repeatedly told it would be once we\u2019d exhausted other options.<\/p><p>They told us our number one priority was to find another job. Every role we found had significant downsides. Lower pay, lower title, RTO, or various other things.<\/p><p>It was clear they wanted us to take a different role we could quit later. My management wanted to retain the headcount, but couldn\u2019t do layoffs.<\/p><p>October 16th I asked my VP for the severance I was told would be available. He let me know HR wasn\u2019t aware of what he was doing and he would have to get approval. It would take some time.<\/p><p>Every week for the next 2 1\/2 months I asked for an update on my employment and severance package. I was either ghosted or given a variety of excuses. It\u2019s now December 30th and I\u2019m currently still employed by Amazon.<\/p><p>It hasn\u2019t only been happening to my team. This has been happening in multiple areas as Amazon silently sacks people without being required to give them severance or announce layoffs. I\u2019ve heard similar tactics being used at other companies\u2013mostly large companies\u2013and it\u2019ll only continue in 2024 as they make decisions that drive short term profits over all else.<\/p><\/div><div class=\"bg-blue-100 dark:bg-gray-900 container px-4 py-12 mx-auto max-w-4xl grid grid-cols-1 md:grid-cols-2 gap-4 items-center\" readability=\"36\"><div readability=\"6\"><p>Follow me<\/p><p class=\"opacity-60\">Here's where I hang out in social media<\/p><\/div><\/div>",
        "created_at": "2024-01-01T12:17:12+01:00"
    },
    {
        "tag":"Programming/Angular",
        "category":"Technical",
        "title": "Angular 17 \u2013 Kalter Kaffee oder Renaissance?",
        "url": "https:\/\/www.codecentric.de\/wissens-hub\/blog\/angular-17-eine-echte-renaissance",
        "content": "<p>Gef\u00fchlt war es lange still rund um das Frontend-Framework Angular. Echte Innovationen blieben aus und man konnte das Gef\u00fchl nicht loswerden, dass Vue.js und React mit all ihren Derivaten den Vorsprung zu Angular uneinholbar weit ausbauen. Doch mit Version 17 hat das Angular-Team im Rahmen einer Keynote die Renaissance des Frameworks \u00f6ffentlichkeitswirksam ausgerufen. Kein Wunder, denn der einstige Platzhirsch muss sich gegen Frameworks wie Remix oder Next.js durchsetzen. Diese haben sich stark am Markt positioniert und \u00fcberzeugen durch ihre hohe Feature-Dichte sowie \u201eDeveloper Experience\u201c; und nicht zuletzt durch den Hype, der nicht selten in wellenartiger Form durch die JavaScript-Community schwappt. Mit \u201ethe next big thing\u201c wurde zuletzt seitens Next.js die Grenze zwischen Server- und Clientseite noch st\u00e4rker verw\u00e4ssert. In den sozialen Medien ist von der Wiederbelebung der Skriptsprache PHP die Rede (die nie weg war und immer noch gut 75 % der Webseiten betreibt) und von SQL-Queries im Frontend. \u201eAnarchie!\u201c, oder \u201eBack to the roots!\u201c, m\u00f6chte man schreien, doch soll dies nicht Thema dieses Artikels sein.<\/p><p>Angular hat einiges aufzuholen. Damals\u2122, als die inh\u00e4renten Performance-Probleme von AngularJS so gravierend wurden, hat man sich bei Google dazu entschieden, das Framework von Grund auf neu zu denken und zu implementieren. Dank der starken Strukturierung mittels Konzepten wie Komponenten, Pipes, Modulen, Services, etc. sowie dem Aufsatz von TypeScript als Basis f\u00fchlte sich das Entwickeln von Webanwendungen nicht mehr \u201ehacky\u201c an. Angular hatte es geschafft, dass Webanwendungen \u201centerprisy\u201d wurden und gleichzeitig die \u201eDeveloper Experience\u201c signifikant verbessert wurde.<\/p><p>Was damals \u201efancy\u201c und \u201eawesome\u201c war, wird heute jedoch vermehrt als einengend und beh\u00e4big wahrgenommen. Viele Schritte, die bei der Implementierung von Angular-Applikationen notwendig sind, f\u00fchlen sich zu starr an und hinken im Vergleich zu React und Vue.js weit hinterher. Und vom Server-Side Rendering (SSR) wollen wir an dieser Stelle gar nicht sprechen.<\/p><p>Doch mit Angular 17 wird alles anders, ja, besser als es jemals war! Das Team rund um das Framework hat versprochen, wieder mehr Traktion in die Weiterentwicklung bekommen zu haben und dies in Form einer Keynote pr\u00e4sentiert, welche im Folgenden einmal (inklusive Einw\u00fcrfen aus dem Chat) kommentiert wird.<\/p><h2>Alles neu macht der November<\/h2><p>Die Zeitenwende unterstreicht das Angular-Team mit dem Redesign der Marke \u201eAngular\u201c. Sie kommt ab sofort in peppigen Farben daher und wirkt fast wie die Corporate Identity von Adobes Creative Cloud. Das neue Corporate Design von Angular trifft den aktuellen Zeit- und Designgeist und steht somit den konkurrierenden Frameworks in nichts nach. Passend zum Redesign wurde unter der URL\u00a0<a class=\"c5\" href=\"https:\/\/angular.dev\" title=\"https:\/\/angular.dev\" target=\"_blank\">https:\/\/angular.dev<\/a> ein komplett neu gestalteter Auftritt gelauncht, der nicht nur eine von Grund auf neu geschriebene Dokumentation mit neuen Tutorials, Essentials sowie einem Angular Playground beinhaltet, sondern auch einen Dark Mode vorweisen kann. Abgesehen davon, dass in der Vorstellung reichlich viel Aufmerksamkeit auf den Dark Mode gelegt wurde, wirkt der Auftritt leichtgewichtig und frisch. Das Team rund um Angular wirft also nicht nur mit Worth\u00fclsen um sich, sondern tischt richtig auf, bekennt sich zum Open Source-Gedanken (\u201eWe love open source\u201c) und betont den Fokus auf Performance sowie Developer Experience, was sie unter anderem in den folgenden Punkten unter Beweis stellen.<\/p><h2>SSR &amp; Hydration<\/h2><p>Mit Version 17 holt Angular die Funktionalit\u00e4ten rund um Server-Side Rendering sowie Hydration in die native Angular CLI, sodass kein Umweg mehr \u00fcber Angular Universal Pakete mehr notwendig ist. Man hat erkannt, dass SSR mittlerweile einen\u00a0<em>wesentlichen Aspekt<\/em> bei der Auswahl eines Frameworks darstellt. Mit dem Flag\u00a0--ssr lassen sich nun Apps erstellen, die von Hause aus bereits Server-Side Rendering aktiviert haben, au\u00dferdem ist Hydration grunds\u00e4tzlich immer mit an Board. Mit der neuen Angular Version sind SSR und Hydration als \u201eproduction-ready\u201c ausgerufen worden. Dies ist ein \u00fcberf\u00e4lliger Schritt, da Next.js oder Nuxt hier bereits die Nase vorn haben. Dennoch verspricht auch hier das Team hinter Angular erneut, dass ein Hauptaugenmerk auf Performance gelegt wurde, sodass die Build-Zeit f\u00fcr Server-Bundles reduziert sowie der SSR-Dev-Server schneller l\u00e4uft.<\/p><p>Neben den bereits implementierten Features wurden jedoch auch weitere Punkte genannt, an denen noch gearbeitet wird und die in folgenden Versionen zu erwarten sind. So nimmt sich das Team eine erweiterte Kompatibilit\u00e4t zu Nicht-Node.js-Umgebungen vor. Progressive Hydration sowie HTTP-Streaming stehen ebenso auf der Agenda wie die Verbesserung der Developer Experience rund um parametrisierte Routen.<\/p><h2>New control flow syntax<\/h2><p>Als Randnotiz ist die fortschreitende Integration von Signals in Angular zu vermerken. Hier wird aktuell die API \u201eglatt gezogen\u201c und an der \u201ezoneless experience\u201c gearbeitet. Eine totale Abkehr von Observables und Rxjs wird es jedoch so bald nicht geben. Die Empfehlung lautet weiterhin, dass f\u00fcr einen Stream von Events (bspw. Websockets) Observables die richtige Wahl sind und Komponent-State \u00fcber Signals abgebildet werden sollte, da letztere eine feingranulare Kontrolle des States erlauben und gleichzeitig leichtgewichtiger daher kommen.<\/p><p>Durch Einflussnahme der Angular-Community (\u201eThe star-syntax is not well understood and quirky\u201c oder auch \u201eimports of NgIf and NgFor are annoying\u201c) sowie der Umstellung auf Signals wurde die Kontrollflusssyntax umgekrempelt. Zwar funktionieren die altbekannten Direktiven\u00a0ngIf oder\u00a0ngFor weiterhin wie gewohnt, jedoch wird eine neue, alternative Syntax angeboten, die wesentliche Verbesserungen in Darstellung, Fehlerhandling sowie Performance umsetzt. Letzteres wird durch die Implementierung der Syntax in Angular-Core erreicht, sodass diese nicht mehr wie\u00a0ngFor oder\u00a0ngIf \u00fcber die Angular public API funktionieren, sondern auf low-level-APIs zur\u00fcckgreifen k\u00f6nnen und somit den Call-Stack erheblich reduzieren. Gerade for-loops k\u00f6nnen laut Aussage des Teams bis zu 90 % schneller sein.<\/p><h3>ngIf wird zu @if<\/h3><p>Neben einem\u00a0<em>else<\/em>-Zweig wurde der neuen Kontrollflusssyntax auch ein <em>else if\u00a0<\/em>spendiert:<\/p><pre>&lt;section&gt;\n    @if (user.loggedIn) {\n        &lt;app-dashboard \/&gt;\n    } \n    @else if(user.role === \u2018admin\u2019) {\n        &lt;app-dashboard \/&gt;\n        &lt;app-admin-controls \/&gt;\n    } \n    @else {\n        &lt;app-login \/&gt;\n    }   \n&lt;\/section&gt;<\/pre><h3>ngFor wird zu @for<\/h3><p>Was bisher als best-practice bei\u00a0ngFor galt, ist nun Pflicht: die Angabe von\u00a0track. Allein hier\u00fcber ist es Angulars Change Detection m\u00f6glich, einiges an Performance-Boost herauszuholen. Zwischen den Zeilen h\u00f6rt man die marketing-wirksame Zahl von 90 % Geschwindigkeitszuwachs. Jedoch wird im direkten Interview bei \u201econtrol flow heavy components\u201c eher von 30 % schnellem Rendering gesprochen.<\/p><pre>&lt;section&gt;\n    @for (user of userList; track user) {\n        &lt;app-card [data]=\"user\" \/&gt;\n    } \n    @empy {\n        &lt;p&gt;There were no items in the list&lt;\/p&gt;\n    }\n&lt;\/section&gt;<\/pre><h3>ngSwitch wird @switch<\/h3><p>Auch\u00a0NgSwitch kann durch die neue Syntax ersetzt werden. Viel ge\u00e4ndert hat sich hier jedoch nicht. Es gibt weiterhin kein \u201efall-through\u201c, sodass ein\u00a0break oder\u00a0return nicht notwendig ist.<\/p><pre>&lt;section&gt;\n        @switch (condition) {\n           @case (caseA) {\n             &lt;app-component-a \/&gt;\n           }\n           @case (caseB) {\n             &lt;app-component-b \/&gt;\n           }\n           @default {\n             &lt;app-component-c \/&gt;\n           }\n        }\n&lt;\/section&gt;<\/pre><h2>Deferred loading<\/h2><p>Deferred loading (oder auch lazy loading, aber das Angular-Team sagt \u201ewe prefer\u00a0<em>defer<\/em>\u201c) ist ein alter Hut und wird sp\u00e4testens seit Version 15 unterst\u00fctzt. Mit Version 17 wurde jedoch auch hier einiges neu gedacht und neu gemacht: Dank der neuen Control-Flow-Syntax ist es nun m\u00f6glich, im Template einen Block mit dem Namen <em>@defer<\/em> zu definieren, der je nach \u00fcbergebener Konfiguration den Inhalt nachl\u00e4dt. In Kombination mit <em>@placeholder, @loading<\/em> und <em>@error<\/em> (siehe Code weiter unten) lassen sich tats\u00e4chlich gut lesbare Template-Strukturen abbilden. Jeder dieser Bl\u00f6cke, so wird stets betont, l\u00e4uft non-blocking. Hei\u00dft also, dass alle Komponenten, die nicht in einem <em>@defer-<\/em>Block enthalten sind, sofort gerendert werden. Spannend an dieser Stelle ist zu erw\u00e4hnen, dass gerade das Thema Testing hervorgehoben wurde und man \u00fcber TestBed geeignete APIs aufrufen kann, die das Abtesten der einzelnen <em>@defer-<\/em>Bl\u00f6cke zulassen.<\/p><pre>&lt;section&gt;\n        \u2026\n        @defer (on viewport)  {\n                &lt;large-component \/&gt;\n        }\n        &lt;huge-component \/&gt;\n        &lt;enormous-component \/&gt;\n        \u2026\n&lt;\/section&gt;<\/pre><h3>Custom Triggers<\/h3><p>Lade Komponenten entweder, wenn sie in den sichtbaren Bereich gescrollt werden oder aber, wenn ein expliziter Trigger gesetzt wird.\u00a0<\/p><pre>&lt;section&gt;\n        &lt;button #trigger (click)=\u201dload=true\u201d&gt;\n                Load Component\n        &lt;\/button&gt;\n        @defer (on viewport(trigger); when load == true) {\n         &lt;large-component \/&gt;\n        }\n&lt;\/section&gt;<\/pre><h3>Prefetch<\/h3><p>Lade die Komponente asynchron im Hintergrund:\u00a0<\/p><pre>&lt;section&gt;\n        \u2026\n        @defer (prefetch on immediate; \n  prefetch when val === true)  {\n                &lt;large-component \/&gt;\n        }\n        \u2026\n&lt;\/section&gt;<\/pre><h3>Placeholder, Loading and Error<\/h3><p>Die Verwendung von <em>@defer<\/em> erlaubt die Kombination mit <em>@placeholder<\/em> f\u00fcr die Anzeige einer leichtgewichtigen Komponente, die als Platzhalter gerendert wird. <em>@loading<\/em> wiederum erlaubt das Einbinden einer Komponente,die w\u00e4hrend des Ladevorgangs gerendert wird. Sollte die in <em>@defer<\/em> eingebundene Komponente nicht ohne Fehler geladen werden, kann \u00fcber <em>@error<\/em> ein Fehler angezeigt werden.<\/p><pre>&lt;section&gt;\n        &lt;button #trigger&gt;...&lt;\/button&gt;\n        @defer (on interaction(trigger)) {\n                &lt;recommended-movies \/&gt;\n        } \n        @placeholder (minimum 500ms) {\n                &lt;img src=\u201dplaceholder.png\u201d\/&gt;\n        } \n        @loading (after 500ms; minimum 1s) {\n                &lt;spinner \/&gt;\n        } \n        @error {\n                &lt;p&gt;Oops, something went wrong.&lt;\/p&gt;\n        }\n&lt;\/section&gt;<\/pre><h2>Easier to use: Standalone components<\/h2><p>Das bisherige Manko einer kleineren Angular-Applikation war das Management des App Modules und das Anreichern um neue Komponenten oder Pipes. Seit Version 17 ist dies nicht mehr zwingend erforderlich, da sie Standalone by default sind. Das mag zwar auf den ersten Blick nicht gravierend erscheinen, ist aber in Kombination mit den Self-Closing Tags aus Version 16 ein echter Gewinner und hilft dabei, sch\u00f6ne und wartbare Komponenten zu erstellen. Mit dem n\u00e4chsten Schritt, der \u00d6ffnung f\u00fcr Esbuild und Vite, werden weitere Konstanten des Angular-Systems (Webpack) angez\u00e4hlt. Nicht zuletzt durch SSR, sondern auch die generell langsame Build-Zeit wurden bereits in der vergangenen Version Vorbereitungen getroffen, Vite und Esbuild in der CLI-Build-Pipeline einzusetzen. Mit Version 17 sind diese auch nicht mehr Bestandteil der Developer Preview, sondern Standard im Toolset. Im Zuge der Umstellung erzeugt die Build-Pipeline nun ESM-Module und aktiviert die Hydration von Haus aus. Dennoch wurde auch Wert darauf gelegt, dass Webpack und somit bestehende Umgebungen weiterhin funktionieren, wobei klar kommuniziert wird, dass dies nicht mehr empfohlen wird.<\/p><h2>Summa summarum<\/h2><p>Angular 17 ist ein spannendes und wegweisendes Release des Frameworks, das zeigen kann und muss, ob die angek\u00fcndigten \u00c4nderungen signifikanten Einfluss auf die Developer Experience haben und die L\u00fccke zu anderen Frameworks schlie\u00dfen oder zumindest verkleinern kann. Sollte auch nur ansatzweise die Build-Zeit erheblich beschleunigt werden (bis zu 90 % wird vielfach genannt), ist eine wesentliche Verbesserung in Kombination mit schnellerer (testgetriebener) Entwicklung von Komponenten zu erwarten. Was durch die Keynote definitiv erreicht wurde, ist Fokus und eine Reihe von gro\u00dfen Versprechen in die Runde aller Webentwickelnden. Davon abgesehen wurde auch ein nicht zu vernachl\u00e4ssigender Hype erzeugt, der sich in Form von Diskussionen, Beitr\u00e4gen, positiven wie negativen Kommentaren und weiteren \u00c4u\u00dferungen manifestiert. Kurzum: Angular hat es geschafft, wieder von sich reden zu machen und die Roadmap f\u00fcr die Zukunft auszurollen.<\/p>",
        "created_at": "2023-12-30T21:01:20+01:00"
    },
    {
        "tag":"DataScience/LLM/Programming",
        "category":"Technical",
        "title": "Answer questions about your documents with OpenAI and Pinecone",
        "url": "https:\/\/www.codecentric.de\/wissens-hub\/blog\/answer-questions-about-your-documents-with-openai-and-pinecone",
        "content": "<head><title\/><\/head><body class=\"readabilityBody\" readability=\"357\">\n<p>In recent years, large language models (LLMs) have made remarkable progress in interacting with humans, showcasing their ability to answer a wide array of questions. Trained on publicly accessible internet content, these models have broad knowledge across many different topics. However, they are limited to information that was available to them during training and thus fail to answer any questions about specific content from your personal documents.<\/p>\n<p>In this article, we will have a look at how we can overcome this limitation, by combining <a href=\"https:\/\/platform.openai.com\/docs\/overview\">OpenAI<\/a>'s chat completion model with a <a href=\"https:\/\/www.pinecone.io\/\">Pinecone<\/a> vector database. We will first have a look at the general approach and then go into detail and implement an intelligent question answering system in Python, using the APIs of both OpenAI and Pinecone, which enables the LLM to provide useful responses to inquiries about personal documents.<\/p>\n<h2 id=\"approach\">Approach<\/h2>\n<p>The core idea of the approach is to use OpenAI's chat completion model to answer questions about our documents. To do so, we create a prompt that includes the question and the documents and asks the model to answer the question based on the text contents of these documents. With this naive idea we face the obstacle that the prompt we can provide as input to the chat completion model is limited in length. Meanwhile, we might have a large number of documents, whose contents combined exceed this limit. Hence, we first have to filter the documents to find the most relevant ones for the question, in order to reduce the length to the prompt below the limit.<\/p>\n<p>To find the documents that are relevant for a question we make use of text embeddings. Text embeddings are high-dimensional numerical vectors that represent the meaning of a text in such a way, that semantically related texts are close to each other in the vector space. We can use an embedding model to embed all our documents, resulting in a vector for each document. While different embedding models are available, we will use an embedding model provided by OpenAI via the API.<\/p>\n<p>The resulting vectors are then stored in a vector database. Vector databases are designed to store and efficiently query large amounts of vectors. Querying uses an algorithmic approach to find the nearest neighboring vectors to a given query vector in a database index, using different distance metrics. In this case, we will use the vector database provided by Pinecone, which is a managed vector database service.<\/p>\n<p><img alt=\"Document embedding process: The document is embedded using OpenAI and the resulting vector is stored in the Pinecone database\" srcset=\"\/_next\/image?url=https%3A%2F%2Fmedia.graphassets.com%2Foutput%3Dformat%3Awebp%2FnkjDpDIlQX6LJagAyvSN&amp;w=640&amp;q=75 1x, \/_next\/image?url=https%3A%2F%2Fmedia.graphassets.com%2Foutput%3Dformat%3Awebp%2FnkjDpDIlQX6LJagAyvSN&amp;w=1920&amp;q=75 2x\" src=\"https:\/\/www.codecentric.de\/_next\/image?url=https%3A%2F%2Fmedia.graphassets.com%2Foutput%3Dformat%3Awebp%2FnkjDpDIlQX6LJagAyvSN&amp;w=1920&amp;q=75\" width=\"610\" height=\"350\" decoding=\"async\" data-nimg=\"future\" loading=\"lazy\" class=\"c2\" referrerpolicy=\"no-referrer\"\/><\/p>\n<p>With a Pinecone index filled with our embedded documents, we can now ask questions about their contents. To do so, we first embed the question using the same embedding model that we used for the documents. This results in a vector representation of the question, which should be close to the vectors of semantically related documents, that could provide the information required to answer the question.<\/p>\n<p>By querying the Pinecone index with the embedding vector of the question, we retrieve the nearest document vectors in the database. We load the texts of the found documents and combine them to a prompt for the chat completion model, along with the question text. Then, we put the prompt into the chat completion model, which will return an answer to our question based on the texts of the relevant documents.<\/p>\n<p><img alt=\"Query process: the query is embedded and the nearest document vector in Pinecone is used to build the prompt for OpenAI\" srcset=\"\/_next\/image?url=https%3A%2F%2Fmedia.graphassets.com%2Foutput%3Dformat%3Awebp%2FhnayhIzMSHGnTNnJ3nSC&amp;w=1200&amp;q=75 1x, \/_next\/image?url=https%3A%2F%2Fmedia.graphassets.com%2Foutput%3Dformat%3Awebp%2FhnayhIzMSHGnTNnJ3nSC&amp;w=3840&amp;q=75 2x\" src=\"https:\/\/www.codecentric.de\/_next\/image?url=https%3A%2F%2Fmedia.graphassets.com%2Foutput%3Dformat%3Awebp%2FhnayhIzMSHGnTNnJ3nSC&amp;w=3840&amp;q=75\" width=\"1109\" height=\"412\" decoding=\"async\" data-nimg=\"future\" loading=\"lazy\" class=\"c2\" referrerpolicy=\"no-referrer\"\/><\/p>\n<h2>Implementation<\/h2>\n<p>We will look at an implementation of the question answering approach in a small Python script. You can find the entire code for the demo on <a href=\"https:\/\/github.com\/LukasL97\/openai-pinecone-search\">GitHub<\/a>.<\/p>\n<h3 id=\"set-up-openai-and-pinecone\">Set up OpenAI and Pinecone<\/h3>\n<p>We first need to create accounts and API keys for <a href=\"https:\/\/platform.openai.com\/\">OpenAI<\/a> and <a href=\"https:\/\/app.pinecone.io\/\">Pinecone<\/a> in the respective developer consoles. Assuming the API keys are stored in environment variables, we can initialize the OpenAI and Pinecone clients in our python script.<\/p>\n<pre class=\"language-python c5\">1import os\n2import openai\n3import pinecone\n45openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n6pinecone.init(api_key=os.getenv(\"PINECONE_API_KEY\"), environment='gcp-starter')<\/pre>\n<h3 id=\"create-the-pinecone-index\">Create the Pinecone index<\/h3>\n<p>We can create a Pinecone index either in the Pinecone console or programmatically using the client. Here, we will do the latter to create an index named <em>document-search-index<\/em>. The index can be configured with regard to different parameters, most notably the dimensions and the metric. The dimensions specify the size of the vectors that we will store in the index. As our embedding model will use vectors of size 1,536, we set the dimensions accordingly. For the metric we have the choice between <em>cosine<\/em>, <em>dotproduct<\/em> and <em>euclidean<\/em>. <a href=\"https:\/\/platform.openai.com\/docs\/guides\/embeddings\/which-distance-function-should-i-use\">As the OpenAI documentation recommends using cosine similarity<\/a>, we will use the <em>cosine<\/em> metric.<\/p>\n<pre class=\"language-python c5\">1pinecone_index_name = 'document-search-index'2pinecone.create_index(pinecone_index_name, \n metric='cosine', dimension=1536)<\/pre>\n<p>We can also configure the index with regard to the number of pods and pod type. However, in the free tier we are limited to a single pod and pod type. The <a href=\"https:\/\/docs.pinecone.io\/docs\/choosing-index-type-and-size\">Pinecone documentation<\/a> explains how the index can be configured in more detail.<\/p>\n<h3 id=\"embed-and-store-your-documents-in-the-pinecone-index\">Embed and store your documents in the Pinecone index<\/h3>\n<p>Now that we have created the Pinecone index, we can embed and store our documents in the index. First, we need to load our documents from the disk. In this case, we assume that the documents are stored in a directory named <em>data<\/em>. The documents are loaded from the directory and returned as a list of dicts, consisting of the <em>title<\/em> (i.e. the filename without ending) and the <em>content<\/em>.<\/p>\n<pre class=\"language-python c5\">1import os\n23def load_documents():4    documents = []\n5    documents_path = 'data'6    for filename in os.listdir(documents_path):\n7        file_path = os.path.join(documents_path, filename)\n8        with open(file_path, 'r', encoding='utf-8') as file:\n9            content = file.read()\n10        documents.append({'title': filename.split('.')[0], 'content': content})\n11    return documents<\/pre>\n<p>Next, we need a function to embed the content of a document using OpenAI's embedding model. The OpenAI client offers an endpoint for that, which allows us to specify an embedding model. We use the model <em>text-embedding-ada-002<\/em>, which is <a href=\"https:\/\/platform.openai.com\/docs\/guides\/embeddings\/embedding-models\">recommended by OpenAI<\/a> at the time of writing this article. The model generates embedding vectors of size 1,536.<\/p>\n<pre class=\"language-python c5\">1import openai.embeddings_utils\n23def get_embedding_vector_from_openai(text):4  \n  return openai.embeddings_utils.get_embedding(text, engine='text-embedding-ada-002')<\/pre>\n<p>With the documents and the embedding function, we are now able to fill our Pinecone index with the embedded documents. The <em>upsert<\/em> method of the Pinecone client expects a list of vectors with <em>id<\/em>, <em>values<\/em> (i.e. the actual vector), and <em>metadata<\/em>. The <em>id<\/em> is a unique identifier for each vector in the index and can be used to query a particular vector. As we won't need this in our use case, we simply set a random value as <em>id<\/em>. The <em>metadata<\/em> can be any additional information that we want to store together with the vector. In this case, we store the title of the document as <em>metadata<\/em>.<\/p>\n<pre class=\"language-python c5\">1import time\n2import uuid\n34def fill_pinecone_index(documents):5    index = pinecone.Index(pinecone_index_name)\n6    for doc in documents:\n7        try:\n8            embedding_vector = get_embedding_vector_from_openai(doc['content'])\n9            data = pinecone.Vector(\n10                id=str(uuid.uuid4()),\n11                values=embedding_vector,\n12                metadata={'title': doc['title']}\n13            )\n14            index.upsert([data])\n15            print(f'Embedded and inserted document with title ' + doc['title'])\n16            time.sleep(1)\n17        except:\n18            print(f'Could not embed and insert document with title ' + doc['title'])\n1920documents = load_documents()\n21fill_pinecone_index(documents)<\/pre>\n<p>You may notice that we have added a <code>time.sleep(1)<\/code> after each embedding and insertion. This is in order to avoid a rate limit error from OpenAI, which only allows a certain number of tokens to be embedded per minute. Further, the embedding model we use is currently limited to texts of up to 8,191 input tokens, which may not be enough for all documents in <em>data<\/em>. In this case, we simply skip the embedding and insertion of these document, so that not all our documents will end up as vectors in the index. If you have large documents with a lot of text, you may want to consider splitting them into smaller chunks and embed those individually.<\/p>\n<p>To answer questions about our documents, we will first find the relevant ones by querying the Pinecone index, and then use these documents and combine them with the question into a prompt for the OpenAI chat completion endpoint, asking the model to answer the question based on the given text.<\/p>\n<p>To retrieve the relevant documents, we simply embed the question using the same model that we used to embed the documents. Then, we query the index with this embedding vector, which will retrieve the top <em>k<\/em> similar vectors in the index. We set <em>k<\/em> to 1 in this case, as we only answer the question based on a single document. You may want to use a larger value for <em>k<\/em> to enable the system to take multiple documents into account, if that is required for your use case. We fetch the title of the document from the metadata, which will enable us to retrieve the document from the disk.<\/p>\n<pre class=\"language-python c5\">1def query_pinecone_index(query):2    index = pinecone.Index(pinecone_index_name)\n3    query_embedding_vector = get_embedding_vector_from_openai(query)\n4    response = index.query(\n5        vector=query_embedding_vector,\n6        top_k=1,\n7        include_metadata=True8    )\n9    return response['matches'][0]['metadata']['title']<\/pre>\n<p>We use the title of the document to retrieve the document content from the disk:<\/p>\n<pre class=\"language-python c5\">1def load_document_content(title):2    documents_path = 'data'3   \n file_path = os.path.join(documents_path, title + '.txt')\n4    with open(file_path, 'r', encoding='utf-8') as file:\n5        content = file.read()\n6    return content<\/pre>\n<p>We further implement a helper method, to combine the document and the question into a prompt for the chat completion model:<\/p>\n<pre class=\"language-python c5\">1def create_prompt(question, document_content):2  \n  return 'You are given a document and a question. Your task is to answer the question based on the document.\n\n' \\\n3           'Document:\\n\\n' \\\n4           f'{document_content}\\n\\n' \\\n5           f'Question: {question}'<\/pre>\n<p>Finally, we can use the OpenAI client to ask the chat completion model to answer the question based on the document. We set the model to <em>gpt-3.5-turbo-16k<\/em>. This is not the state-of-the-art model, but it is currently cheaper than the different variants of <em>gpt-4<\/em> and should be sufficient for this use case. The <em>16k<\/em> version of the model allows for up to 16,385 tokens, which allows us to put long texts into the prompt. We pass a list of messages to the chat completion model, which consists of the conversation up to this point. As we start a new conversation, our list consists of a single user message with our prompt as content. The model returns a list of completion choices, which could be more than one if specified in the request. As we did not specify a value, it defaults to only a single completion. We extract the message content of the completion, which contains the answer to our prompt.<\/p>\n<pre class=\"language-python c5\">1def get_answer_from_openai(question):2    relevant_document_title = query_pinecone_index(question)\n3    print(f'Relevant document title: {relevant_document_title}')\n4    document_content = load_document_content(relevant_document_title)\n5    prompt = create_prompt(question, document_content)\n6    print(f'Prompt:\\n\\n{prompt}\\n\\n')\n7    completion = openai.ChatCompletion.create(\n8        model='gpt-3.5-turbo-16k',\n9        messages=[{\n10            'role': 'user',\n11            'content': prompt\n12        }]\n13    )\n14    return completion.choices[0].message.content\n1516question = input('Enter a question: ')\n17answer = get_answer_from_openai(question)\n18print(answer)<\/pre>\n<h3 id=\"examples\">Examples<\/h3>\n<p>Now we can ask questions about information from out documents and retrieve an answer from OpenAI. Using around 800 Wikipedia articles about different topics as our example documents, we try out the following question:<\/p>\n<blockquote readability=\"5\">\n<p>What role does the president play in the political system of Angola?<\/p>\n<\/blockquote>\n<p>The Pinecone index yields the vector of the document <em>Politics of Angola<\/em> as most similar to the embedded query. Using this document in our prompt enables OpenAI to answer the question correctly:<\/p>\n<blockquote readability=\"39\">\n<p>The president in the political system of Angola holds almost absolute power. They are the head of state and head of government, as well as the leader of the winning party or coalition. The president appoints and dismisses members of the government, members of various courts, the Governor and Vice-Governors of the Nacional Angolan Bank, the General-Attorney and their deputies, the Governors of the provinces, and many other key positions in the government, military, police, intelligence, and security organs. The president is also responsible for defining the policy of the country and has the power to promulgate laws and make edicts. However, the president is not directly involved in making laws.<\/p>\n<\/blockquote>\n<p>While this is already impressive, it has to be mentioned that we cheated a little, as we used Wikipedia articles as our documents for testing the system. As the OpenAI model was trained on publicly available internet content, it is likely that it has seen this exact article and would have been able to answer the question anyway, even without receiving the document as part of the input prompt. Hence, we will have a look at what happens when we ask about a topic that has not been seen by the OpenAI model before.<\/p>\n<p>To this end, I made up an article about the fictional chemist <em>Jacob Miller<\/em>, who was born in 1850, discovered a chemical element in 1886, which he named <em>Jacobium<\/em>, received the Nobel Prize for his findings in 1901, and died in 1932. You can find the article in the <a href=\"https:\/\/github.com\/LukasL97\/openai-pinecone-search\/blob\/main\/data\/Jacob%20Miller%20(Chemist).txt\"><em>data<\/em> directory of the GitHub repository<\/a>. The document was embedded along with all other documents and inserted into the Pinecone index. Now, let's have a look at what happens when we ask about Jacob Miller:<\/p>\n<blockquote readability=\"5\">\n<p>In which year did the discoverer of the chemical element Jacobium win the nobel prize?<\/p>\n<\/blockquote>\n<p>As we would expect, the Pinecone index yields the vector of the document <em>Jacob Miller (Chemist)<\/em> as most similar to the embedded query. Using this document in the prompt enables the chat completion model to provide the correct answer to the question:<\/p>\n<blockquote readability=\"5\">\n<p>The discoverer of the chemical element Jacobium won the Nobel Prize in Chemistry in the year 1901.<\/p>\n<\/blockquote>\n<h2 id=\"conclusion\">Conclusion<\/h2>\n<p>As demonstrated in this article, we can combine large language models and vector databases to build intelligent question answering systems, that are able to answer questions about specific content from our documents. With managed services like OpenAI and Pinecone, we can easily build such systems without having to worry about training our own models or setting up and maintaining our own vector database.<\/p>\n<p>As a disclaimer, I want to note that data privacy should be considered when using the approach presented in this article to build a question answering system on documents that contain sensitive information. <a href=\"https:\/\/openai.com\/enterprise-privacy\">While OpenAI claims, that they do not train their models on inputs provided via the API<\/a>, you might still want to consider alternatives when working with private or enterprise data. Instead of using OpenAI, you could use <a href=\"https:\/\/github.com\/eugeneyan\/open-llms\">one of the many open source LLMs<\/a> and host it yourself, which naturally comes with significantly greater expenses.<\/p>\n<p>Further, Pinecone is not the only vector database service available. An interesting alternative is the open-source database solution <a href=\"https:\/\/www.trychroma.com\/\">Chroma<\/a>, which actually comes with its own embedding model. Instead of inserting and querying embedding vectors into the database, Chroma allows us to work with the texts directly, rendering the use of OpenAI's embedding model unnecessary. Another alternative is <a href=\"https:\/\/faiss.ai\/index.html\">Faiss<\/a>, which is an open-source library for efficient GPU-driven vector similarity search. Compared to Pinecone, Faiss lacks the ability to store vectors, as it is only a vector index, not a vector database. Both Chroma and Faiss require you to host a database yourself, which make them somewhat less convenient to use than the managed Pinecone database.<\/p>\n<\/body>",
        "created_at": "2023-11-14T21:35:16+01:00"
    },
    {
        "tag":"DataScience/LLM/Application",
        "category":"Strategy",
        "title": "My \"bicycle of the mind\" moment with LLMs",
        "url": "https:\/\/birchtree.me\/blog\/the-critical-reason-llms-arent-the-new-nfts\/",
        "content": "<p>Critics of LLM-based products like ChatGPT, Claude, Midjourney, and other such products like to brush them off as just this year\u2019s version of NFTs. They\u2019re crypto bullshit being peddled by the same jokers who are just out there to stow disinformation and make a quick buck.<\/p><p>I won\u2019t deny that there are some perpetual fad-chasers out there, and of course those people have been like a moth to a flame to both of these trends, but fad-chasing scam artists flock to everything, whether it has legs or not.<\/p><p>Quick anecdote: I\u2019m the resident video guy at work, and if we need a video made to show off a new product or to train people on how something works, I\u2019m the guy they go to. Today we were on a very tight deadline and we needed a video about something I\u2019m not a subject matter expert in. What we decided to do was have someone who was an expert in the area record a walkthrough of the thing, riff without a script as they went into their laptop mic, and send me their video which would be the basis for my higher production value video. I took the video and fed it into MacWhisper, which locally generated a high quality transcript of what was said in their video. I was able to feed that transcript into another LLM to ask some questions such as:<\/p><ul><li>Summarize this text<\/li>\n<li>Write an outline of the important events<\/li>\n<li>Clean up the text in the 3rd paragraph<\/li>\n<\/ul><p>There were some other prompts, but the gist is that I was able to use this LLM to help look at the walkthrough from a few angles very quickly. Mind you I didn\u2019t take any of this as gospel, and I did watch the video as well and ultimately wrote the script myself, but these tools helped me get my hands around the content with such speed that I simply would not have been able to do on my own. It was a real \u201cbicycle of the mind\u201d sort of moment for me when I found myself seamlessly using these tools to enhance my work and to do things at a high quality in less time than ever before.<\/p><p>What I find notable about this sort of interaction with LLMs is that I didn\u2019t use them to skip over things like understanding the thing I was working on or to deliver a worse product faster, indeed I was using these tools to accelerate my understanding and deliver an exceptionally high quality product.<\/p><p>Back to the start of this post, I think it\u2019s notable that at no point in the above story did I mention that I convinced someone else to use the tools I was using. The value I get from these products does not rely one iota on whether other people are using them. The fact I was able to perform my job faster and better than before today is completely removed from whether other people are using these tools as well.<\/p><p>This is in stark contrast to the crypto\/NFT run which was entirely reliant on more and more people getting into the game. An NFT on its own is utterly useless, you need to get someone else to want it more so that you can turn a profit. The workflow I used today to be better at my job? I don\u2019t care at all if anyone else is doing what I\u2019m doing, I get value either way. Honestly, if other people don\u2019t adopt these tools and I find ways to use them well, that just gives me a competitive advantage.<\/p><p>So yes, the same jokers who want to show you how to get rich quick with the latest fad are drawn to this year\u2019s trendiest technology, just like they were to crypto and just like they will be to whatever comes next. All I would suggest is that you look back on the <a href=\"https:\/\/birchtree.me\/blog\/lets-pretend-the-last-20-years-never-happened\/\">history of Birchtree<\/a> where I absolutely roasted crypto for a year before it just felt mean to beat a clearly dying horse, and recognize that the people who are enthusiastic about LLMs aren\u2019t just fad-chasing hype men.<\/p>",
        "created_at": "2023-10-13T06:22:17+02:00"
    }

]